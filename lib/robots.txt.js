import fs from 'fs'
import { generateEnhancedRobotsTxt } from './seo/robotsEnhanced'

export function generateRobotsTxt(props) {
  const { NOTION_CONFIG } = props
  const isEnhanced = process.env.NEXT_PUBLIC_SEO_ROBOTS_ENHANCED !== 'false'
  
  if (isEnhanced) {
    // 使用增强版robots.txt生成器
    try {
      console.log('Generating enhanced robots.txt...')
      generateEnhancedRobotsTxt(props)
    } catch (error) {
      // 如果增强版失败，回退到简单版本
      console.warn('Enhanced robots.txt generation failed, using fallback:', error.message)
      generateFallbackRobotsTxt(props)
    }
  } else {
    console.log('Using fallback robots.txt generation...')
    generateFallbackRobotsTxt(props)
  }
}

// 保留原有的简单版本作为回退
function generateFallbackRobotsTxt(props) {
  const { siteInfo, NOTION_CONFIG } = props
  
  // 优先使用blog.config.js中的配置
  const { siteConfig } = require('./config')
  const LINK = siteConfig('LINK', 'https://www.shareking.vip', NOTION_CONFIG)
  
  // 确保链接不以斜杠结尾
  const baseUrl = LINK && LINK.endsWith('/') ? LINK.slice(0, -1) : LINK
  
  const content = `# Robots.txt for ${baseUrl}
# Generated by NotionNext

# Allow all web crawlers to access all content
User-agent: *
Allow: /

# Disallow access to admin and private areas
Disallow: /admin/
Disallow: /api/
Disallow: /dashboard/
Disallow: /_next/
Disallow: /static/

# Disallow access to search result pages
Disallow: /search?*
Disallow: /search/*

# Allow access to important directories
Allow: /images/
Allow: /css/
Allow: /js/
Allow: /fonts/

# Specific rules for major search engines
User-agent: Googlebot
Allow: /
Crawl-delay: 1

User-agent: Bingbot
Allow: /
Crawl-delay: 1

User-agent: Baiduspider
Allow: /
Crawl-delay: 2

# Block AI training bots
User-agent: GPTBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: anthropic-ai
Disallow: /

User-agent: Claude-Web
Disallow: /

User-agent: Bytespider
Disallow: /

User-agent: Amazonbot
Disallow: /

User-agent: Applebot-Extended
Disallow: /

User-agent: Google-Extended
Disallow: /

User-agent: meta-externalagent
Disallow: /

# Block problematic bots
User-agent: AhrefsBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /

User-agent: SemrushBot
Disallow: /

# Host declaration
Host: ${baseUrl}

# Sitemap locations
Sitemap: ${baseUrl}/sitemap.xml
Sitemap: ${baseUrl}/sitemap-images.xml
Sitemap: ${baseUrl}/sitemap-pages.xml
Sitemap: ${baseUrl}/sitemap-posts.xml
`
  
  try {
    fs.mkdirSync('./public', { recursive: true })
    fs.writeFileSync('./public/robots.txt', content)
    console.log('✅ Robots.txt generated successfully')
  } catch (error) {
    console.warn('⚠️ Cannot write robots.txt file:', error.message)
    // 在vercel运行环境是只读的，这里会报错；
    // 但在vercel编译阶段、或VPS等其他平台这行代码会成功执行
  }
}
