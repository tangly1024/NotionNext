# Robots.txt éªŒè¯å™¨

ä¸€ä¸ªå…¨é¢çš„ robots.txt éªŒè¯ç³»ç»Ÿï¼Œç”¨äºç¡®ä¿ NotionNext é¡¹ç›®çš„ robots.txt æ–‡ä»¶ç¬¦åˆ RFC 9309 æ ‡å‡†ã€æœç´¢å¼•æ“è¦æ±‚å’Œ SEO æœ€ä½³å®è·µã€‚

## åŠŸèƒ½ç‰¹æ€§

- âœ… **æ ¼å¼éªŒè¯** - æ£€æŸ¥æ–‡ä»¶æ ¼å¼ã€ç¼–ç ã€è¯­æ³•ç»“æ„
- âœ… **å†…å®¹éªŒè¯** - éªŒè¯æŒ‡ä»¤å†…å®¹ã€URL æ ¼å¼ã€è·¯å¾„è§„åˆ™
- âœ… **æ ‡å‡†åˆè§„** - æ£€æŸ¥ RFC 9309 æ ‡å‡†åˆè§„æ€§
- âœ… **SEO ä¼˜åŒ–** - æ£€æŸ¥ SEO æœ€ä½³å®è·µå’Œæœç´¢å¼•æ“ç‰¹å®šè§„åˆ™
- âœ… **å¤šæ ¼å¼æŠ¥å‘Š** - æ”¯æŒæ§åˆ¶å°ã€JSONã€HTML æ ¼å¼è¾“å‡º
- âœ… **CLI å·¥å…·** - å‘½ä»¤è¡Œç•Œé¢ï¼Œæ”¯æŒ CI/CD é›†æˆ
- âœ… **å¯é…ç½®** - çµæ´»çš„é…ç½®é€‰é¡¹å’Œè§„åˆ™å¼•æ“

## å¿«é€Ÿå¼€å§‹

### ä½¿ç”¨ npm è„šæœ¬

```bash
# åŸºæœ¬éªŒè¯
npm run validate:robots

# JSON æ ¼å¼è¾“å‡º
npm run validate:robots:json

# ä¸¥æ ¼æ¨¡å¼
npm run validate:robots:strict
```

### ä½¿ç”¨ CLI å·¥å…·

```bash
# åŸºæœ¬éªŒè¯
node scripts/validate-robots.js

# æŒ‡å®šæ–‡ä»¶
node scripts/validate-robots.js --file custom-robots.txt

# JSON è¾“å‡º
node scripts/validate-robots.js --format json

# é™é»˜æ¨¡å¼
node scripts/validate-robots.js --quiet

# æŸ¥çœ‹å¸®åŠ©
node scripts/validate-robots.js --help
```

### ç¼–ç¨‹æ¥å£

```javascript
import { RobotsValidator } from './lib/seo/robotsValidator.js'

// åˆ›å»ºéªŒè¯å™¨å®ä¾‹
const validator = new RobotsValidator({
  filePath: 'public/robots.txt',
  outputFormat: 'json',
  strict: false
})

// æ‰§è¡ŒéªŒè¯
const result = await validator.validate()

// ç”ŸæˆæŠ¥å‘Š
const report = validator.generateReport(result)
console.log(report)
```

## é…ç½®é€‰é¡¹

### åŸºæœ¬é…ç½®

- `filePath` - robots.txt æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼š'public/robots.txt'ï¼‰
- `strict` - ä¸¥æ ¼æ¨¡å¼ï¼ˆé»˜è®¤ï¼šfalseï¼‰
- `outputFormat` - è¾“å‡ºæ ¼å¼ï¼š'console', 'json', 'html'ï¼ˆé»˜è®¤ï¼š'console'ï¼‰
- `verbose` - è¯¦ç»†è¾“å‡ºï¼ˆé»˜è®¤ï¼štrueï¼‰

### éªŒè¯é€‰é¡¹

- `checkAccessibility` - æ£€æŸ¥ URL å¯è®¿é—®æ€§ï¼ˆé»˜è®¤ï¼štrueï¼‰
- `validateSitemaps` - éªŒè¯ sitemap æ–‡ä»¶ï¼ˆé»˜è®¤ï¼štrueï¼‰
- `checkSEO` - SEO æ£€æŸ¥ï¼ˆé»˜è®¤ï¼štrueï¼‰

### ç½‘ç»œé…ç½®

- `timeout` - ç½‘ç»œè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆé»˜è®¤ï¼š5000msï¼‰
- `userAgent` - ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²ï¼ˆé»˜è®¤ï¼š'RobotsValidator/1.0'ï¼‰

## éªŒè¯ç±»åˆ«

### 1. æ ¼å¼éªŒè¯
- æ–‡ä»¶ç¼–ç æ£€æŸ¥ï¼ˆUTF-8ï¼‰
- è¡Œç»“æŸç¬¦éªŒè¯
- è¯­æ³•ç»“æ„æ£€æŸ¥
- æŒ‡ä»¤æ ¼å¼éªŒè¯

### 2. å†…å®¹éªŒè¯
- User-agent æŒ‡ä»¤éªŒè¯
- Allow/Disallow è§„åˆ™æ£€æŸ¥
- Sitemap URL éªŒè¯
- Host å£°æ˜æ£€æŸ¥

### 3. æ ‡å‡†åˆè§„
- RFC 9309 åˆè§„æ€§æ£€æŸ¥
- æŒ‡ä»¤ä¼˜å…ˆçº§éªŒè¯
- è§„åˆ™å†²çªæ£€æµ‹

### 4. SEO ä¼˜åŒ–
- æœç´¢å¼•æ“ç‰¹å®šè§„åˆ™
- AI æœºå™¨äººå±è”½æ£€æŸ¥
- é‡è¦è·¯å¾„å¯è®¿é—®æ€§

## æŠ¥å‘Šæ ¼å¼

### æ§åˆ¶å°æŠ¥å‘Š
```
============================================================
ğŸ¤– ROBOTS.TXT éªŒè¯æŠ¥å‘Š
============================================================
âœ… éªŒè¯çŠ¶æ€: é€šè¿‡
ğŸ“Š æ€»åˆ†: 85/100
ğŸ“ˆ ç»Ÿè®¡: 8 é€šè¿‡, 2 è­¦å‘Š, 0 é”™è¯¯

âœ… æ ¼å¼éªŒè¯ (100/100)
  âœ“ æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥: robots.txt æ–‡ä»¶å­˜åœ¨ä¸”ä¸ä¸ºç©º
  âœ“ User-agent æŒ‡ä»¤æ£€æŸ¥: æ‰¾åˆ° User-agent æŒ‡ä»¤
  âœ“ è®¿é—®è§„åˆ™æ£€æŸ¥: æ‰¾åˆ°è®¿é—®æ§åˆ¶è§„åˆ™
============================================================
```

### JSON æŠ¥å‘Š
```json
{
  "timestamp": "2025-07-28T14:54:23.314Z",
  "validator": "RobotsValidator",
  "version": "1.0.0",
  "result": {
    "isValid": true,
    "score": 85,
    "summary": {
      "totalChecks": 10,
      "passed": 8,
      "warnings": 2,
      "errors": 0
    },
    "categories": { ... },
    "recommendations": [ ... ],
    "metadata": { ... }
  }
}
```

## æ•°æ®æ¨¡å‹

### ValidationResult
éªŒè¯ç»“æœçš„ä¸»è¦æ•°æ®ç»“æ„ï¼ŒåŒ…å«ï¼š
- `isValid` - éªŒè¯æ˜¯å¦é€šè¿‡
- `score` - æ€»åˆ†ï¼ˆ0-100ï¼‰
- `summary` - ç»Ÿè®¡æ‘˜è¦
- `categories` - å„éªŒè¯ç±»åˆ«ç»“æœ
- `recommendations` - æ”¹è¿›å»ºè®®
- `metadata` - å…ƒæ•°æ®ä¿¡æ¯

### ValidationCategory
éªŒè¯ç±»åˆ«ï¼ŒåŒ…å«ï¼š
- `name` - ç±»åˆ«åç§°
- `passed` - æ˜¯å¦é€šè¿‡
- `score` - ç±»åˆ«åˆ†æ•°
- `checks` - æ£€æŸ¥é¡¹åˆ—è¡¨

### ValidationCheck
å•ä¸ªéªŒè¯æ£€æŸ¥ï¼ŒåŒ…å«ï¼š
- `id` - æ£€æŸ¥ID
- `name` - æ£€æŸ¥åç§°
- `status` - çŠ¶æ€ï¼š'pass', 'warning', 'error'
- `message` - æ£€æŸ¥æ¶ˆæ¯
- `suggestion` - ä¿®å¤å»ºè®®
- `severity` - ä¸¥é‡ç¨‹åº¦

## CI/CD é›†æˆ

### GitHub Actions ç¤ºä¾‹

```yaml
name: Validate Robots.txt
on: [push, pull_request]

jobs:
  validate-robots:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-node@v2
        with:
          node-version: '20'
      - run: npm install
      - run: npm run validate:robots
```

### é€€å‡ºä»£ç 
- `0` - éªŒè¯é€šè¿‡
- `1` - éªŒè¯å¤±è´¥æˆ–å‘ç”Ÿé”™è¯¯

## æ‰©å±•æ€§

### è‡ªå®šä¹‰éªŒè¯è§„åˆ™
```javascript
// å°†åœ¨åç»­ä»»åŠ¡ä¸­å®ç°
class CustomValidator {
  validate(content, context) {
    // è‡ªå®šä¹‰éªŒè¯é€»è¾‘
  }
}

validator.addPlugin(new CustomValidator())
```

### è§„åˆ™é…ç½®
```javascript
const validator = new RobotsValidator({
  rules: {
    'require-host': { enabled: true, severity: 'warning' },
    'https-sitemaps': { enabled: true, severity: 'error' },
    'block-ai-bots': { enabled: true, severity: 'info' }
  }
})
```

## å¼€å‘çŠ¶æ€

å½“å‰å®ç°äº†æ ¸å¿ƒéªŒè¯å™¨æ¶æ„å’ŒåŸºç¡€åŠŸèƒ½ï¼š

- âœ… æ ¸å¿ƒéªŒè¯å™¨ç±» `RobotsValidator`
- âœ… æ•°æ®æ¨¡å‹ç±»ï¼ˆValidationResult, ValidationCategory, ValidationCheck, Recommendationï¼‰
- âœ… åŸºç¡€é…ç½®ç®¡ç†å’Œé€‰é¡¹å¤„ç†
- âœ… éªŒè¯æµç¨‹æ¡†æ¶å’Œé”™è¯¯å¤„ç†æœºåˆ¶
- âœ… å¤šæ ¼å¼æŠ¥å‘Šç”Ÿæˆï¼ˆæ§åˆ¶å°ã€JSONã€HTMLï¼‰
- âœ… CLI å·¥å…·å’Œ npm è„šæœ¬é›†æˆ
- âœ… å®Œæ•´çš„å•å…ƒæµ‹è¯•å¥—ä»¶

### åç»­ä»»åŠ¡
- ğŸ”„ æ ¼å¼éªŒè¯å™¨å®ç°
- ğŸ”„ å†…å®¹éªŒè¯å™¨å®ç°
- ğŸ”„ æ ‡å‡†åˆè§„éªŒè¯å™¨å®ç°
- ğŸ”„ SEO ä¼˜åŒ–éªŒè¯å™¨å®ç°
- ğŸ”„ é«˜çº§åŠŸèƒ½å’Œæ€§èƒ½ä¼˜åŒ–

## è®¸å¯è¯

MIT License - è¯¦è§ LICENSE æ–‡ä»¶

## è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

---

**NotionNext Robots.txt Validator v1.0.0**